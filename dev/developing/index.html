<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Developing · moment_kinetics</title><meta name="title" content="Developing · moment_kinetics"/><meta property="og:title" content="Developing · moment_kinetics"/><meta property="twitter:title" content="Developing · moment_kinetics"/><meta name="description" content="Documentation for moment_kinetics."/><meta property="og:description" content="Documentation for moment_kinetics."/><meta property="twitter:description" content="Documentation for moment_kinetics."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">moment_kinetics</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../chebyshev/">Chebyshev tranform via Fourier transform</a></li><li><a class="tocitem" href="../constraints_on_normalized_distribution_function/">Constraints on normalized distribution function</a></li><li><a class="tocitem" href="../debugging-hints/">Debugging</a></li><li class="is-active"><a class="tocitem" href>Developing</a><ul class="internal"><li><a class="tocitem" href="#Dependencies"><span>Dependencies</span></a></li><li><a class="tocitem" href="#Revise.jl"><span>Revise.jl</span></a></li><li><a class="tocitem" href="#Input-options-and-defaults"><span>Input options and defaults</span></a></li><li><a class="tocitem" href="#Array-types"><span>Array types</span></a></li><li><a class="tocitem" href="#Timings"><span>Timings</span></a></li><li><a class="tocitem" href="#Parallelization"><span>Parallelization</span></a></li><li><a class="tocitem" href="#Bounds-checking"><span>Bounds checking</span></a></li><li><a class="tocitem" href="#parallel_io_section"><span>Parallel I/O</span></a></li><li><a class="tocitem" href="#Package-structure"><span>Package structure</span></a></li></ul></li><li><a class="tocitem" href="../external_sources_notes/">External sources</a></li><li><a class="tocitem" href="../fokker_planck_notes/">Fokker Planck collision operator</a></li><li><a class="tocitem" href="../geometry/">Magnetic Geometry</a></li><li><a class="tocitem" href="../getting_started/">Getting started</a></li><li><a class="tocitem" href="../input_options/">Input Options</a></li><li><a class="tocitem" href="../jacobian_matrices/">Jacobian matrix calculations</a></li><li><a class="tocitem" href="../known_issues/">Known Issues</a></li><li><a class="tocitem" href="../machine_setup_notes/"><code>machine_setup</code> notes</a></li><li><a class="tocitem" href="../manual_setup/">Manual setup</a></li><li><a class="tocitem" href="../manufactured_solution_test_examples/">List of Manufactured Solutions Test TOML inputs</a></li><li><a class="tocitem" href="../moment_kinetic_equations/">Moment kinetic equations</a></li><li><a class="tocitem" href="../moment_kinetic_equations_electrons/">Moment kinetic equations for electrons</a></li><li><a class="tocitem" href="../parameter_scans/">Parameter scans</a></li><li><a class="tocitem" href="../post_processing_notes/">Post processing</a></li><li><a class="tocitem" href="../shared_memory_debugging/">Shared memory debugging</a></li><li><a class="tocitem" href="../timestepping/">Timestepping</a></li><li><a class="tocitem" href="../wall_boundary_conditions/">Wall boundary conditions with moment constraints</a></li><li><a class="tocitem" href="../zz_MKJacobianUtils/"><code>MKJacobianUtils</code></a></li><li><a class="tocitem" href="../zz_advection/"><code>advection</code></a></li><li><a class="tocitem" href="../zz_alpha_advection/"><code>alpha_advection</code></a></li><li><a class="tocitem" href="../zz_analysis/"><code>analysis</code></a></li><li><a class="tocitem" href="../zz_array_allocation/"><code>array_allocation</code></a></li><li><a class="tocitem" href="../zz_bgk/"><code>bgk</code></a></li><li><a class="tocitem" href="../zz_boundary_conditions/"><code>boundary_conditions</code></a></li><li><a class="tocitem" href="../zz_calculus/"><code>calculus</code></a></li><li><a class="tocitem" href="../zz_charge_exchange/"><code>charge_exchange</code></a></li><li><a class="tocitem" href="../zz_chebyshev/"><code>chebyshev</code></a></li><li><a class="tocitem" href="../zz_clenshaw_curtis/"><code>clenshaw_curtis</code></a></li><li><a class="tocitem" href="../zz_collision_frequencies/"><code>collision_frequencies</code></a></li><li><a class="tocitem" href="../zz_command_line_options/"><code>command_line_options</code></a></li><li><a class="tocitem" href="../zz_communication/"><code>communication</code></a></li><li><a class="tocitem" href="../zz_constants/"><code>constants</code></a></li><li><a class="tocitem" href="../zz_continuity/"><code>continuity</code></a></li><li><a class="tocitem" href="../zz_coordinates/"><code>coordinates</code></a></li><li><a class="tocitem" href="../zz_debugging/"><code>debugging</code></a></li><li><a class="tocitem" href="../zz_derivatives/"><code>derivatives</code></a></li><li><a class="tocitem" href="../zz_electron_fluid_equations/"><code>electron_fluid_equations</code></a></li><li><a class="tocitem" href="../zz_electron_kinetic_equation/"><code>electron_kinetic_equation</code></a></li><li><a class="tocitem" href="../zz_electron_vpa_advection/"><code>electron_vpa_advection</code></a></li><li><a class="tocitem" href="../zz_electron_z_advection/"><code>electron_z_advection</code></a></li><li><a class="tocitem" href="../zz_em_fields/"><code>em_fields</code></a></li><li><a class="tocitem" href="../zz_energy_equation/"><code>energy_equation</code></a></li><li><a class="tocitem" href="../zz_external_sources/"><code>external_sources</code></a></li><li><a class="tocitem" href="../zz_file_io/"><code>file_io</code></a></li><li><a class="tocitem" href="../zz_finite_differences/"><code>finite_differences</code></a></li><li><a class="tocitem" href="../zz_fokker_planck/"><code>fokker_planck</code></a></li><li><a class="tocitem" href="../zz_fokker_planck_calculus/"><code>fokker_planck_calculus</code></a></li><li><a class="tocitem" href="../zz_fokker_planck_test/"><code>fokker_planck_test</code></a></li><li><a class="tocitem" href="../zz_force_balance/"><code>force_balance</code></a></li><li><a class="tocitem" href="../zz_fourier/"><code>fourier</code></a></li><li><a class="tocitem" href="../zz_gauss_legendre/"><code>gauss_legendre</code></a></li><li><a class="tocitem" href="../zz_geo/"><code>geo</code></a></li><li><a class="tocitem" href="../zz_gyroaverages/"><code>gyroaverages</code></a></li><li><a class="tocitem" href="../zz_initial_conditions/"><code>initial_conditions</code></a></li><li><a class="tocitem" href="../zz_input_structs/"><code>input_structs</code></a></li><li><a class="tocitem" href="../zz_interpolation/"><code>interpolation</code></a></li><li><a class="tocitem" href="../zz_ionization/"><code>ionization</code></a></li><li><a class="tocitem" href="../zz_jacobian_matrices/"><code>jacobian_matrices</code></a></li><li><a class="tocitem" href="../zz_krook_collisions/"><code>krook_collisions</code></a></li><li><a class="tocitem" href="../zz_lagrange_polynomials/"><code>lagrange_polynomials</code></a></li><li><a class="tocitem" href="../zz_load_data/"><code>load_data</code></a></li><li><a class="tocitem" href="../zz_loop_ranges_struct/"><code>loop_ranges_struct</code></a></li><li><a class="tocitem" href="../zz_looping/"><code>looping</code></a></li><li><a class="tocitem" href="../zz_makie_post_processing/"><code>makie_post_processing</code></a></li><li><a class="tocitem" href="../zz_manufactured_solns/"><code>manufactured_solns</code></a></li><li><a class="tocitem" href="../zz_maxwell_diffusion/"><code>maxwell_diffusion</code></a></li><li><a class="tocitem" href="../zz_moment_constraints/"><code>moment_constraints</code></a></li><li><a class="tocitem" href="../zz_moment_kinetics/"><code>moment_kinetics</code></a></li><li><a class="tocitem" href="../zz_moment_kinetics_input/"><code>moment_kinetics_input</code></a></li><li><a class="tocitem" href="../zz_moment_kinetics_structs/"><code>moment_kinetics_structs</code></a></li><li><a class="tocitem" href="../zz_neutral_r_advection/"><code>neutral_r_advection</code></a></li><li><a class="tocitem" href="../zz_neutral_vz_advection/"><code>neutral_vz_advection</code></a></li><li><a class="tocitem" href="../zz_neutral_z_advection/"><code>neutral_z_advection</code></a></li><li><a class="tocitem" href="../zz_nonlinear_solvers/"><code>nonlinear_solvers</code></a></li><li><a class="tocitem" href="../zz_numerical_dissipation/"><code>numerical_dissipation</code></a></li><li><a class="tocitem" href="../zz_plot_MMS_sequence/"><code>plot_MMS_sequence</code></a></li><li><a class="tocitem" href="../zz_plot_sequence/"><code>plot_sequence</code></a></li><li><a class="tocitem" href="../zz_plots_post_processing/"><code>plots_post_processing</code></a></li><li><a class="tocitem" href="../zz_quadrature/"><code>quadrature</code></a></li><li><a class="tocitem" href="../zz_r_advection/"><code>r_advection</code></a></li><li><a class="tocitem" href="../zz_reference_parameters/"><code>reference_parameters</code></a></li><li><a class="tocitem" href="../zz_runge_kutta/"><code>runge_kutta</code></a></li><li><a class="tocitem" href="../zz_source_terms/"><code>source_terms</code></a></li><li><a class="tocitem" href="../zz_species_input/"><code>species_input</code></a></li><li><a class="tocitem" href="../zz_time_advance/"><code>time_advance</code></a></li><li><a class="tocitem" href="../zz_timer_utils/"><code>timer_utils</code></a></li><li><a class="tocitem" href="../zz_type_definitions/"><code>type_definitions</code></a></li><li><a class="tocitem" href="../zz_utils/"><code>utils</code></a></li><li><a class="tocitem" href="../zz_velocity_grid_transforms/"><code>velocity_grid_transforms</code></a></li><li><a class="tocitem" href="../zz_velocity_moments/"><code>velocity_moments</code></a></li><li><a class="tocitem" href="../zz_vpa_advection/"><code>vpa_advection</code></a></li><li><a class="tocitem" href="../zz_vperp_advection/"><code>vperp_advection</code></a></li><li><a class="tocitem" href="../zz_z_advection/"><code>z_advection</code></a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Developing</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Developing</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/mabarnes/moment_kinetics" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/mabarnes/moment_kinetics/blob/master/docs/src/developing.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Developing"><a class="docs-heading-anchor" href="#Developing">Developing</a><a id="Developing-1"></a><a class="docs-heading-anchor-permalink" href="#Developing" title="Permalink"></a></h1><h2 id="Dependencies"><a class="docs-heading-anchor" href="#Dependencies">Dependencies</a><a id="Dependencies-1"></a><a class="docs-heading-anchor-permalink" href="#Dependencies" title="Permalink"></a></h2><p>If you need to add a dependency, start the REPL with the <code>moment_kinetics</code> package activated (see <a href="#moment_kinetics">above</a>), enter <code>pkg&gt;</code> mode (press <code>]</code>) and then to add, for example, the <code>FFTW.jl</code> package enter</p><pre><code class="nohighlight hljs">(moment_kinetics) pkg&gt; add FFTW</code></pre><p>This should take care of adding the package (<code>FFTW</code>) to the <code>Project.toml</code> and <code>Manifest.toml</code> files.</p><h2 id="Revise.jl"><a class="docs-heading-anchor" href="#Revise.jl">Revise.jl</a><a id="Revise.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Revise.jl" title="Permalink"></a></h2><p>When working on the code, one way to avoid waiting for everything to recompile frequently is to load the Revise.jl package</p><pre><code class="nohighlight hljs">julia&gt; using Revise</code></pre><p><code>Revise.jl</code> will recompile each edited function/method as needed, so it is possible to keep a REPL session open and avoid long recompilation. <code>moment_kinetics</code> can be run fairly conveniently from the REPL</p><pre><code class="nohighlight hljs">julia&gt; using moment_kinetics
julia&gt; run_moment_kinetics(input)</code></pre><p>where <code>input</code> is a <code>Dict()</code> containing any non-default options desired. Input can also be loaded from a TOML file passing the filaname as a String to the second argument, e.g.</p><pre><code class="nohighlight hljs">julia&gt; run_moment_kinetics(&quot;input.toml&quot;)</code></pre><p>It might be convenient to add <code>using Revise</code> to your <code>startup.jl</code> file (<code>~/julia/config/startup.jl</code>) so it&#39;s always loaded.</p><h2 id="Input-options-and-defaults"><a class="docs-heading-anchor" href="#Input-options-and-defaults">Input options and defaults</a><a id="Input-options-and-defaults-1"></a><a class="docs-heading-anchor-permalink" href="#Input-options-and-defaults" title="Permalink"></a></h2><p>The input is read from a <code>.toml</code> file. It is also written to the output HDF5 (or NetCDF) file, after all defaults are applied, both as a TOML-formatted String and as a tree of HDF5 variables.</p><div class="admonition is-warning" id="Warning-df7f3aea8aa4e53a"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-df7f3aea8aa4e53a" title="Permalink"></a></header><div class="admonition-body"><p>Neither TOML nor HDF5 have a &#39;null&#39; type, so there is no convenient way to store Julia&#39;s <code>nothing</code> when writing to TOML or HDF5.  Therefore <code>nothing</code> should not be used as a default for any input option. If the code should use <code>nothing</code> as a default for some setting, that is fine, but must be done after the input is read, and not stored in the <code>input_dict</code>.</p></div></div><div class="admonition is-warning" id="Parallel-I/O-consistency-a1b1e7db78b03cbb"><header class="admonition-header">Parallel I/O consistency<a class="admonition-anchor" href="#Parallel-I/O-consistency-a1b1e7db78b03cbb" title="Permalink"></a></header><div class="admonition-body"><p>To ensure consistency between all MPI ranks in the order of reads and/or writes when using Parallel I/O, all dictionary types used to store options must be either <code>OrderedDict</code> or <code>SortedDict</code>, so that their order of entries is deterministic (which is not the case for <code>Dict</code>, which instead optimises for look-up speed). This should mostly be taken care of by using <code>moment_kinetics</code>&#39;s <code>OptionsDict</code> type (which is an alias for <code>OrderedDict{String,Any}</code>). We also need to sort the input after it is read by <code>TOML</code>, which is taken care of by <a href="../zz_input_structs/#moment_kinetics.input_structs.convert_to_sorted_nested_OptionsDict-Tuple{AbstractDict}"><code>moment_kinetics.input_structs.convert_to_sorted_nested_OptionsDict</code></a>. See also <a href="#parallel_io_section">Parallel I/O</a>.</p></div></div><h2 id="Array-types"><a class="docs-heading-anchor" href="#Array-types">Array types</a><a id="Array-types-1"></a><a class="docs-heading-anchor-permalink" href="#Array-types" title="Permalink"></a></h2><p>Most arrays in <code>moment_kinetics</code> are declared using a custom array type <a href="../zz_type_definitions/#moment_kinetics.type_definitions.MPISharedArray"><code>moment_kinetics.type_definitions.MPISharedArray</code></a>. Most of the time this type is just an alias for <code>Array</code>, and so it needs the same template parameters (see <a href="https://docs.julialang.org/en/v1/manual/arrays/">Julia&#39;s Array documentation</a>) - the data type and the number of dimensions, e.g. <code>MPISharedArray{mk_float,3}</code>. Although these arrays use shared memory, Julia does not know about this. We use <code>MPI.Win_allocate_shared()</code> to allocate the shared memory, then wrap it in an <code>Array</code> in <a href="../zz_communication/#moment_kinetics.communication.allocate_shared-Tuple{Type, Union{Nothing, MPI.Comm}, Bool, Vararg{Union{Pair{Symbol, Int64}, moment_kinetics.moment_kinetics_structs.coordinate}}}"><code>moment_kinetics.communication.allocate_shared</code></a>.</p><p>The reason for using the alias, is that when the shared-memory debugging mode is activated, we instead create arrays using a type <code>DebugMPISharedArray</code>, which allows us to track some debugging information along with the array, see <a href="../shared_memory_debugging/#Shared-memory-debugging">Shared memory debugging</a>, and make <code>MPISharedArray</code> an alias for <code>DebugMPISharedArray</code> instead. The reason for the alias is that if we declared our structs with just <code>Array</code> type, then when debugging is activated we would not be able to store <code>DebugMPISharedArray</code> instances in those structs, and if we declared the structs with <code>AbstractArray</code>, they would not be concretely typed, which could impact performance by creating code that is not &#39;type stable&#39; (i.e. all concrete types are known at compile time).</p><h2 id="Timings"><a class="docs-heading-anchor" href="#Timings">Timings</a><a id="Timings-1"></a><a class="docs-heading-anchor-permalink" href="#Timings" title="Permalink"></a></h2><p>Checking the timings of different parts of the code can be useful to check that performance problems are not introduced. Excessive allocations can also be a sign of type instability (or other problems) that could impact performance. To monitor these things, <code>moment_kinetics</code> uses a <code>TimerOutput</code> object <a href="../zz_timer_utils/#moment_kinetics.timer_utils.global_timer"><code>moment_kinetics.timer_utils.global_timer</code></a>.</p><p>The timings and allocation counts from the rank-0 MPI process are printed to the terminal at the end of a run. The same information is also saved to the output file as a string for quick reference - one way to view this is</p><pre><code class="language-bash hljs">$ h5dump -d /timing_data/global_timer_string my_output_file.moments.h5</code></pre><p>More detailed timing information is saved for each MPI rank into subgroups <code>rank&lt;i&gt;</code> of the <code>timing_data</code> group in the output file. This information can be plotted using <a href="../zz_makie_post_processing/#makie_post_processing.timing_data-Tuple{Vector{Any}}"><code>makie_post_processing.timing_data</code></a>. The plots contain many curves. Filtering out the ones you are not interested in (using the <code>include_patterns</code>, <code>exclude_patterns</code>, and/or <code>ranks</code> arguments) can help, but it still may be useful to have interactive plots which show the label and MPI rank when you hover over a curve. For example</p><pre><code class="language-julia hljs">julia&gt; using makie_post_processing, GLMakie
julia&gt; ri = get_run_info(&quot;runs/my_example_run/&quot;)
julia&gt; timing_data(ri; interactive_figs=:times);</code></pre><p>Here <code>using GLMakie</code> selects the <code>Makie</code> backend that provides interactive plots, and the <code>interactive_figs</code> argument specifies that <code>timing_data()</code> should make an interactive plot (in this case for the execution times).</p><p>Lower level timing data, for example timing MPI and linear-algebra calls, can be enabled by activating &#39;debug timing&#39;. This can be done by re-defining the function <a href="../zz_timer_utils/#moment_kinetics.timer_utils.timeit_debug_enabled-Tuple{}"><code>moment_kinetics.timer_utils.timeit_debug_enabled</code></a> to return <code>true</code> - not the most user-friendly interface (!) but this feature is probably only needed while developing/profiling/debugging.</p><h2 id="Parallelization"><a class="docs-heading-anchor" href="#Parallelization">Parallelization</a><a id="Parallelization-1"></a><a class="docs-heading-anchor-permalink" href="#Parallelization" title="Permalink"></a></h2><p>The code is parallelized at the moment using MPI and shared-memory arrays. Arrays representing the pdf, moments, etc. are shared between all processes. Using shared memory means, for example, we can take derivatives along one dimension while parallelising the other for any dimension without having to communicate to re-distribute the arrays. Using shared memory instead of (in future as well as) distributed memory parallelism has the advantage that it is easier to split up the points within each element between processors, giving a finer-grained parallelism which should let the code use larger numbers of processors efficiently.</p><p>It is possible to use a REPL workflow with parallel code:</p><ul><li>Recommended option is to use <a href="https://github.com/Azrael3000/tmpi">tmpi</a>. This utility (it&#39;s a bash script that uses <code>tmux</code>) starts an mpi program with each process in a separate pane in a single terminal, and mirrors input to all processes simultaneously (which is normally what you want, there are also commands to &#39;zoom in&#39; on a single process).</li><li>Another &#39;low-tech&#39; possibilty is to use something like <code>mpirun -np 4 xterm -e julia --project</code>, but that will start each process in a separate xterm and you would have to enter commands separately in each one. Occasionally useful for debugging when nothing else is available.</li></ul><p>There is no restriction on the number of processes or number of grid points, although load-balancing may be affected - if there are only very few points per process, and a small fraction of processes have an extra grid point (e.g. splitting 5 points over 4 processes, so 3 process have 1 point but 1 process has 2 points), many processes will spend time waiting for the few with an extra point.</p><p>Parallelism is implemented through macros that get the local ranges of points that each process should handle. The inner-most level of nested loops is typically not parallelized, to allow efficient FFTs for derivatives, etc. A loop over one (possibly parallelized) dimension can be written as, for example,</p><pre><code class="nohighlight hljs">@loop_s is begin
    f[is] = ...
end</code></pre><p>These macros can be nested as needed for relatively complex loops</p><pre><code class="nohighlight hljs">@loop_s is begin
    some_setup(is)
    @loop_z iz begin
        @views do_something(f[:,iz,is])
    end
    @loop_z iz begin
        @views do_something_else(f[:,iz,is])
    end
end</code></pre><p>Simpler nested loops can (optionally) be written more compactly</p><pre><code class="nohighlight hljs">@loop_s_z_vpa is iz ivpa begin
    f[ivpa,iz,is] = ...
end</code></pre><p>Which dimensions are actually parallelized by these macros is controlled by the &#39;region&#39; that the code is currently in, as set by the <code>@begin_&lt;dims&gt;_region()</code> macros, where &lt;dims&gt; are the dimensions that will be parallelized in the following region. For example, after calling <code>@begin_s_z_region()</code> loops over species and z will be divided up over the processes in a &#39;block&#39; (currently there is only one block, which contains the whole grid and all the processes being used, as we have not yet implemented distributed-memory parallelism). Every process will loop over all points in the remaining dimensions if the loop macros for those dimensions are called.</p><ul><li><p>The recommended place to put <code>@begin_*_region()</code> calls is at the beginning of a function whose contents should use loops parallelised according to the settings for that region.</p><ul><li>Each <code>@begin_*_region()</code> function checks if the region it would set is already active, and if so returns immediately (doing nothing). This means that <code>@begin_*_region()</code> can (and should) be used to mark a block of code as belonging to that region, and if <code>moment_kinetics</code> is already in that region type, the call will have essentially zero cost.</li><li>In some places it may be necessary to change the region type half way through a function, etc. This is fine.</li><li>When choosing which region type to select, note that all &#39;parallelised dimensions&#39; must be looped over for each operation (otherwise some points may be written more than once), unless some special handling is used (e.g. species dimension <code>s</code> is parallelised, but a conditional like <code>if 1 in loop_ranges[].s</code> is wrapped around code to be executed so that only processes which should handle the point at <code>s=1</code> do anything). It may be more optimal in some places to choose region types that do not parallelise all possible dimensions, to reduce the number of synchronisations that are needed.</li><li>As a matter of style, it is recommended to place <code>@begin_*_region()</code> calls within functions where the loops are (or at most one level above), so that it is not necessary to search back along the execution path of the code to find the most recent <code>@begin_*_region()</code> call, and therefore know what region type is active.</li></ul></li><li><p>In a region after <code>@begin_serial_region()</code>, the rank 0 process in each block will loop over all points in every dimension, and all other ranks will not loop over any.</p></li><li><p>Inside serial regions, the macro <code>@serial_region</code> can also be used to wrap blocks of code so that they only run on rank 0 of the block. This is useful for example to allow the use of array-broadcast expressions during initialization where performance is not critical.</p></li><li><p>To help show how these macros work, a script is provided that print a set of examples where the loop macros are expanded. It can be run from the Julia REPL</p><pre><code class="nohighlight hljs">$ julia --project
               _
   _       _ _(_)_     |  Documentation: https://docs.julialang.org
  (_)     | (_) (_)    |
   _ _   _| |_  __ _   |  Type &quot;?&quot; for help, &quot;]?&quot; for Pkg help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 1.7.0 (2021-11-30)
 _/ |\__&#39;_|_|_|\__&#39;_|  |  Official https://julialang.org/ release
|__/                   |

julia&gt; include(&quot;util/print-macros.jl&quot;)</code></pre><p>or on the command line</p><pre><code class="nohighlight hljs">$ julia --project util/print-macros.jl</code></pre></li></ul><p>The ranges used are stored in a <code>LoopRanges</code> struct in the <code>Ref</code> variable <code>loop_ranges</code> (which is exported by the <code>looping</code> module). The range for each dimension is stored in a member with the same name as the dimension, e.g. <code>loop_ranges[].s</code> for the species. Occasionally it is useful to access the range directly. There are different <code>LoopRanges</code> instances for different parallelization patterns - the instance stored in <code>loop_ranges</code> is updated when <code>begin_*_region()</code> is called. It is possible to find out the current region type (i.e. which dimensions are being parallelized) by looking at <code>loop_ranges[].parallel_dims</code>.</p><div class="admonition is-info" id="Note-c2ce67a7da4b7d59"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-c2ce67a7da4b7d59" title="Permalink"></a></header><div class="admonition-body"><p>The square brackets <code>[]</code> after <code>loop_ranges[]</code> are needed because <code>loop_ranges</code> is a reference to a <code>LoopRanges</code> object <code>Ref{LoopRanges}</code> (a bit like a pointer) - it allows <code>loop_ranges</code> to be a <code>const</code> variable, so its type is always known at compile time, but the actual <code>LoopRanges</code> can be set/modified at run-time.</p></div></div><p>It is also possible to run a block of code in serial (on just the rank-0 member of each block of processes) by wrapping it in a <code>@serial_region</code> macro. This is mostly useful for initialization or file I/O where performance is not critical. For example</p><pre><code class="nohighlight hljs">@serial_region begin
    # Do some initialization
    f .= 0.0
end</code></pre><p>Internally, when the <code>@begin_*_region()</code> macros need to change the region type (i.e. the requested region is not already active), they call <code>_block_synchronize()</code>, which calls <code>MPI.Barrier()</code>. They also switch over the <code>LoopRanges</code> struct contained in <code>looping.loop_ranges</code> as noted above. For optimization, the <code>_block_synchronize()</code> call can be skipped - when it is correct to do so - by passing the argument <code>no_synchronize=true</code> (or some more complicated conditional expression if synchronization is necessary when using some options but not for others).</p><p>If for some reason it is necessary to synchronize explicitly, not by using an <code>@begin_*_region()</code> call, use the <code>@_block_synchronize()</code> macro. This calls <code>_block_synchronize()</code>, but when debugging can also pass in some information (a hash of the file an line number) about the calling site that is used for consistency checking.</p><h3 id="Collision-operator-and-anysv-region"><a class="docs-heading-anchor" href="#Collision-operator-and-anysv-region">Collision operator and <code>anysv</code> region</a><a id="Collision-operator-and-anysv-region-1"></a><a class="docs-heading-anchor-permalink" href="#Collision-operator-and-anysv-region" title="Permalink"></a></h3><p>The Fokker-Planck collision operator requires a special approach to shared-memory parallelisation. There is an outer loop over spatial points. Inside that outer loop there are operations that can benefit from parallelisation over (possibly species and) <span>$v_{\perp}$</span>, or over <span>$v_{\parallel}$</span>, or over both <span>$v_{\perp}$</span> and <span>$v_{\parallel}$</span>, as well as some that do not parallelise over velocity space at all. To deal with this, it is beneficial to parallelise the outer loop over species and spatial dimensions as much as possible, and then within that allow changes between different ways of parallelizing over velocity space.</p><p>The mechanism introduced to allow the type of parallelization just described is the &#39;anysv&#39; (read any-<span>$s$</span>-<span>$v$</span>) region. Before the outer loop of the collision operator <code>@begin_r_z_anysv_region()</code> is used to start the &#39;anysv&#39; parallelization. Then within the <code>@loop_r_z ir iz begin...</code> the functions <code>@begin_anysv_region()</code> (for no parallelization over velocity space), <code>@begin_anysv_s_vperp_region()</code>, <code>@begin_anysv_s_vpa_region()</code>, <code>@begin_anysv_s_vperp_vpa_region()</code>, <code>@begin_anysv_vperp_region()</code>, <code>@begin_anysv_vpa_region()</code> and <code>@begin_anysv_vperp_vpa_region()</code> can be used to parallelize over species and neither velocity space dimension, either velocity space dimension individually, or over both velocity space dimensions together. This is possible because &#39;subblocks&#39; of processes are defined. Each subblock shares the same range of spatial indices, which stay the same throughout the <code>@begin_r_z_anysv_region()</code> section, and are not shared with any other subblock of processes. Because the subblock has an independent set of spatial-indices, when changing the species- and velocity-space parallelization only the processes in the sub-block need to be synchronized which is done by <a href="../zz_communication/#moment_kinetics.communication._anysv_subblock_synchronize-Tuple{Union{Missing, Nothing, UInt64}}"><code>moment_kinetics.communication._anysv_subblock_synchronize</code></a>, which is called when necessary within the <code>@begin_anysv*_region()</code> functions (the whole shared-memory block does not need to be synchronized at once, as would be done by <a href="../zz_communication/#moment_kinetics.communication._block_synchronize-Tuple{Union{Nothing, UInt64}}"><code>moment_kinetics.communication._block_synchronize</code></a>). The processes that share an anysv subblock are all part of the <code>comm_anysv_subblock[]</code> communicator (which is a subset of the processes in the full block, whose communicator is <code>comm_block[]</code>).</p><p>See also notes on debugging the &#39;anysv&#39; parallelisation: <a href="../shared_memory_debugging/#Collision-operator-and-&#39;anysv&#39;-region">Collision operator and &#39;anysv&#39; region</a>.</p><h2 id="Bounds-checking"><a class="docs-heading-anchor" href="#Bounds-checking">Bounds checking</a><a id="Bounds-checking-1"></a><a class="docs-heading-anchor-permalink" href="#Bounds-checking" title="Permalink"></a></h2><p>For best performance (i.e. &#39;production&#39; runs), it is important that bounds checks not be included on array accesses. It should be possible to do this by running <code>julia</code> with the flag <code>--check-bounds=no</code>, but this flag has negative effects on the core Julia code and compiler, and works less well in Julia versions 1.10 and 1.11. As a workaround/alternative, the <code>@loop_*</code> macros described in the previous section wrap the contained code with an <code>@inbounds</code> macro (which disables bounds checks within the block, but the effect of <code>@inbounds</code> does not propagate down into functions called within the block). If performance-critical code that you write is within an <code>@loop</code>, then you do not need to do anything. However if it is not within an <code>@loop</code>, then you should add <code>@inbounds begin ... end</code> around any performance critical code. You can see examples of this being done in <a href="../zz_fokker_planck_calculus/#moment_kinetics.fokker_planck_calculus"><code>moment_kinetics.fokker_planck_calculus</code></a>.</p><h2 id="parallel_io_section"><a class="docs-heading-anchor" href="#parallel_io_section">Parallel I/O</a><a id="parallel_io_section-1"></a><a class="docs-heading-anchor-permalink" href="#parallel_io_section" title="Permalink"></a></h2><p>The code provides an option to use parallel I/O, which allows all output to be written to a single output file even when using distributed-MPI parallelism - this is the default option when the linked HDF5 library is compiled with parallel-I/O support.</p><p>There are a few things to be aware of to ensure parallel I/O works correctly:</p><ul><li>Some operations have to be called simultaneously on all the MPI ranks that have the output file open. Roughly, these are any operations that change the &#39;metadata&#39; of the file, for example opening/closing files, creating variables, extending dimensions of variables, changing attributes of variables. Reading or writing the data from a variable does not have to be done collectively - actually when we write data we ensure that every rank that is writing writes a non-overlapping slice of the array to avoid contention that could slow down the I/O (because one rank has to wait for another) and to avoid slight inconsistencies because it is uncertain which rank writes the data last. For more details see the <a href="https://juliaio.github.io/HDF5.jl/stable/mpi/#Reading-and-writing-data-in-parallel">HDF5.jl documentation</a> and the <a href="https://support.hdfgroup.org/archive/support/HDF5/doc/RM/CollectiveCalls.html">HDF5 documentation</a>.</li><li>One important subtlety is that the <code>Dict</code> type does not guarantee a deterministic order of entries. When you iterate over a <code>Dict</code>, you can get the results in a different order at different times or on different MPI ranks. If we iterated over a <code>Dict</code> to create variables to write to an output file, or to read from a file, then different MPI ranks might (sometimes) get the variables in a different order, causing errors. We therefore use either <code>OrderedDict</code> or <code>SortedDict</code> types for anything that might be written to or read from an HDF5 file.</li></ul><p>If the collective operations are not done perfectly consistently, the errors can be extremely non-obvious. The inconsistent operations may appear to execute correctly, for example because the same number of variables are created, and the metadata may only actually be written from the rank-0 process, but the inconsistency may cause errors later. [JTO, 3/11/2024: my best guess as to the reason for this is that it puts HDF5&#39;s &#39;metadata cache&#39; in inconsistent states on different ranks, and this means that at some later time the ranks will cycle some metadata out of the cache in different orders, and then some ranks will be able to get the metadata from the cache, while others have to read it from the file. The reading from the file requires some collective MPI call, which is only called from some ranks and not others, causing the code to hang.]</p><h2 id="Package-structure"><a class="docs-heading-anchor" href="#Package-structure">Package structure</a><a id="Package-structure-1"></a><a class="docs-heading-anchor-permalink" href="#Package-structure" title="Permalink"></a></h2><p>The structure of the packages in the <code>moment_kinetics</code> repo is set up so that some features, which depend on &#39;heavy&#39; external packages (such as <code>Makie</code>, <code>Plots</code>, and <code>Symbolics</code>, which take a long time to precompile and load) can be optional.</p><p>The structure is set up by the <code>machines/machine_setup.sh</code> script, which prompts the user for input to decide which optional components to include (as well as some settings related to batch job submission on HPC clusters). <code>machine_setup.sh</code> calls several other scripts to do the setup (written as far as possible in Julia). The structure of these scripts is explained in <a href="../machine_setup_notes/#machine_setup-notes"><code>machine_setup</code> notes</a>.</p><p>The intention is that a top-level &#39;project&#39; (defined by a <code>Project.toml</code> file, which is created and populated by <code>machines/machine_setup.sh</code>) is set up in the top-level directory of the repository. The <code>moment_kinetics</code> package itself (which is in the <code>moment_kinetics/</code> subdirectory, defined by its own <code>Project.toml</code> file which is tracked by git), and optionally other post-processing packages, are added to this top-level project using <code>Pkg.develop()</code>.</p><h3 id="Optional-dependencies"><a class="docs-heading-anchor" href="#Optional-dependencies">Optional dependencies</a><a id="Optional-dependencies-1"></a><a class="docs-heading-anchor-permalink" href="#Optional-dependencies" title="Permalink"></a></h3><p>Some capabilities that require optional dependencies are provided using &#39;package extensions&#39; (<a href="https://julialang.org/blog/2023/04/julia-1.9-highlights/#package_extensions">a new feature of Julia in v1.9.0</a>).</p><p>The way we use package extensions is a bit of a hack. Extensions are intended to be activated when an optional dependency (called a &#39;weakdep&#39; by Julia) is loaded, e.g. <code>using moment_kinetics, NCDatasets</code>. This usage pattern is not the most convenient for the way we use <code>moment_kinetics</code> where we would rather just load <code>moment_kinetics</code> and then specify for example <code>binary_format = &quot;netcdf&quot;</code> in the input TOML file. To work around this, the optional dependencies are loaded automatically if they are installed (by calling <code>Base.requires()</code> in the <code>__init__()</code> function of an appropriate sub-module). This is not the way package extensions were intended to be used, and it may be a bit fragile - at the time of writing in January 2024 there would be an error on precompilation if the optional dependencies were added in one order, which went away when the order was reversed. If this causes problems, we might need to consider an alternative, for example adding the optional dependencies to the <code>startup.jl</code> file, instead of trying to auto-load them from within the <code>moment_kinetics</code> package.</p><p>The optional capabilities at the moment are:</p><ul><li>Method of manufactured solutions (MMS) testing - this requires the <code>Symbolics</code> package which is heavy and has a large number of dependencies. It is convenient not to require <code>Symbolics</code> when MMS capability is not being used. The functionality is provided by the <code>manufactured_solns_ext</code> extension. The extension also requires the <code>IfElse</code> package, which is not needed elsewhere in <code>moment_kinetics</code> and so is included as a &#39;weakdep&#39; although <code>IfElse</code> is not a heavy dependency.</li><li>NetCDF output - this requires the <code>NCDatasets</code> package. Although not as heavy as <code>Symbolics</code> or the plotting packages, NetCDF output is not required and not used by default, so it does not hurt to make the dependency optional. As a bonus, importing <code>NCDatasets</code> can sometimes cause linking errors when a local or system installation of HDF5 (i.e. one not provided by the Julia package manager) is used, as <code>NCDatasets</code> (sometimes?) seems to try to link a different version of the library. These errors can be avoided by not enabling NetCDF outut (when HDF5 output is preferred), or allowing Julia to use the HDF5 library provided by its package manager (when NetCDF is preferred, although this would mean that parallel I/O functionality is not available).</li></ul><h3 id="Post-processing-packages"><a class="docs-heading-anchor" href="#Post-processing-packages">Post processing packages</a><a id="Post-processing-packages-1"></a><a class="docs-heading-anchor-permalink" href="#Post-processing-packages" title="Permalink"></a></h3><p>Post processing functionality is provided by separate packages (<code>makie_post_processing</code> and <code>plots_post_processing</code>) rather than by extensions. Extensions are not allowed to define new modules, functions, etc. within the main package, they can only add new methods (i.e. new implementations of the function for a different number of arguments, or different types of the arguments) to functions already defined in the main package. For post-processing, we want to add a lot of new functions, so to use extensions instead of separate packages we would need to define all the function names in the main package, and then separately the implementations in the extension, which would be inconvenient and harder to maintain.</p><p>There are two suggested ways of setting up the post-processing packages:</p><ol><li>For interactive use/development on a local machine, one or both post-processing packages can be added to the top-level project using <code>Pkg.develop()</code>. This is convenient as there is only one project to deal with. Both simulations and post-processing are run using<pre><code class="nohighlight hljs">$ bin/julia --project -O3 &lt;...&gt;</code></pre></li><li>For optimized use on an HPC cluster it is better to set up a separate project for the post-processing package(s). This allows different optimization flags to be used for running simulations (<code>-O3 --check-bounds=no</code>) and for post-processing (<code>-O3</code>). [Note, in particular Makie.jl can have performance problems if run with <code>--check-bounds=no</code>, see <a href="https://github.com/MakieOrg/Makie.jl/issues/3132">here</a>.] Simulations should be run with<pre><code class="nohighlight hljs">$ bin/julia --project -O3 --check-bounds=no &lt;...&gt;</code></pre>and post-processing with<pre><code class="nohighlight hljs">$ bin/julia --project=makie_post_processing -O3 &lt;...&gt;</code></pre>or<pre><code class="nohighlight hljs">$ bin/julia --project=plots_post_processing -O3 &lt;...&gt;</code></pre>This option can also be used on a local machine, if you want to optimise your simulation runs as much as possible by using the <code>--check-bounds=no</code> flag. To do this answer <code>y</code> to the prompt &quot;Would you like to set up separate packages for post processing...&quot; from <code>machines/machine_setup.sh</code>.</li></ol><p>To support option 2, the post-processing packages are located in sub-sub-directories (<code>makie_post_processing/makie_post_processing/</code> and <code>plots_post_processing/plots_post_processing/</code>), so that the separate projects can be created in the sub-directories (<code>makie_post_processing/</code> and <code>plots_post_processing</code>). <code>moment_kinetics</code> and the other dependencies must also be added to the separate projects (the <code>machine_setup.sh</code> script takes care of this).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../debugging-hints/">« Debugging</a><a class="docs-footer-nextpage" href="../external_sources_notes/">External sources »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Thursday 22 January 2026 09:14">Thursday 22 January 2026</span>. Using Julia version 1.11.8.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
