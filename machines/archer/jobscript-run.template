#!/usr/bin/env bash

#SBATCH --nodes=NODES
#SBATCH --cpus-per-task=1
#SBATCH --ntasks-per-node=128
#SBATCH --time=RUNTIME
#SBATCH --account=ACCOUNT
#SBATCH --partition=PARTITION
#SBATCH --qos=QOS
#SBATCH --output=RUNDIRslurm-%j.out

set -e

cd $SLURM_SUBMIT_DIR

# Get setup for Julia
source julia.env

# Workaround as cpus-per-task no longer inherited by srun from sbatch.
# See https://docs.archer2.ac.uk/faq/upgrade-2023/
export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK

echo "running INPUTFILE $(date)"

if [ -e compute-node-temp.julia.tar.bz ]; then
  # Distribute the tar'ed Julia depot to all compute nodes, saving it to the /tmp/ directory which lives in RAM.
  sbcast --compress=none compute-node-temp.julia.tar.bz  /tmp/compute-node-temp.julia.tar.bz
else
  echo "compute-node-temp.julia.tar.bz but is required: run machines/machine_setup.sh to create it."
  exit 1
fi

srun --distribution=block:block --hint=nomultithread --ntasks=$SLURM_NTASKS machines/archer/tmp-depot-wrapper.sh bin/julia -Jmoment_kinetics.so --project -O3 run_moment_kinetics.jl INPUTFILE

echo "finished INPUTFILE $(date)"
