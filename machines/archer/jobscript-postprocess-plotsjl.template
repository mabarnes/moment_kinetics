#!/usr/bin/env bash

#SBATCH --mem=POSTPROCMEMORY
#SBATCH --time=POSTPROCTIME
#SBATCH --account=ACCOUNT
#SBATCH --partition=serial
#SBATCH --qos=serial
#SBATCH --output=RUNDIRslurm-post-%j.out

set -e

cd $SLURM_SUBMIT_DIR

# Get setup for Julia
source julia.env

# Workaround as cpus-per-task no longer inherited by srun from sbatch.
# See https://docs.archer2.ac.uk/faq/upgrade-2023/
export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK

echo "post-processing (with original post_processing) RUNDIR $(date)"

# Set this environment variable to avoid warning messages from Qt when running without a display
export QT_QPA_PLATFORM=offscreen

if [ -e compute-node-temp.julia.tar.bz ]; then
  # Distribute the tar'ed Julia depot to all compute nodes, saving it to the /tmp/ directory which lives in RAM.
  sbcast --compress=none compute-node-temp.julia.tar.bz  /tmp/compute-node-temp.julia.tar.bz

  # Copy the julia directory (found from the name of the executable) and
  # moment_kinetics.so to the compute nodes for efficiency.
  cp -ra $(dirname $(dirname $(cat .julia_default.txt))) /tmp/julia-dir
  cp -a plots_postproc.so  /tmp/plots_postproc.so
else
  echo "compute-node-temp.julia.tar.bz but is required: run machines/machine_setup.sh to create it."
  exit 1
fi

machines/archer/tmp-depot-wrapper.sh /tmp/$USER/julia-dir/bin/julia -J /tmp/$USER/plots_postproc.so --project=plots_post_processing/ run_post_processing.jl RUNDIR

echo "finished post-processing RUNDIR $(date)"
