#!/usr/bin/env bash

#SBATCH --nodes=NODES
#SBATCH --cpus-per-task=1
#SBATCH --ntasks-per-node=256
# As of 5/11/2025 it is important to set `--ntasks-per-socket` explicitly to avoid poor performance on Pitagora.
#SBATCH --ntasks-per-socket=128
#SBATCH --time=RUNTIME
#SBATCH --account=ACCOUNT
#SBATCH --partition=PARTITION
#SBATCH --qos=QOS
#SBATCH --output=RUNDIRslurm-%j.out

set -e

cd $SLURM_SUBMIT_DIR

# Get setup for Julia
source julia.env

echo "running INPUTFILE $(date)"

if [ -e compute-node-temp.julia.tar.bz ]; then
  # Distribute the tar'ed Julia depot to all compute nodes, saving it to the $TMPDIR directory which is local to each node.
  sbcast --compress=none compute-node-temp.julia.tar.bz  $TMPDIR/compute-node-temp.julia.tar.bz

  # Copy the julia directory (found from the name of the executable) and
  # moment_kinetics.so to the compute nodes for efficiency.
  # sbcast only copies files, so (following hint from
  # https://www.ch.cam.ac.uk/computing/slurm-usage) just use srun instead to
  # copy the julia directory - we are probably not too worried about
  # performance here, so should be fine.
  srun --ntasks=NODES --tasks-per-node=1 cp -ra $(dirname $(dirname $(cat .julia_default.txt))) $TMPDIR/julia-dir
  sbcast --compress=none moment_kinetics.so $TMPDIR/moment_kinetics.so
else
  echo "compute-node-temp.julia.tar.bz does not exist but is required: run machines/machine_setup.sh to create it."
  exit 1
fi

srun --hint=nomultithread --distribution=block:block -n $SLURM_NTASKS machines/pitagora/tmp-depot-wrapper.sh $TMPDIR/$USER/julia-dir/bin/julia -J $TMPDIR/$USER/moment_kinetics.so --project -O3 RUNSCRIPT --restart INPUTFILE RESTARTFROM

echo "finished INPUTFILE $(date)"
